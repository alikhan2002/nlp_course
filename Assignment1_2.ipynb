{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8527cb34-5086-41d2-9797-48b7ce614a15",
   "metadata": {},
   "source": [
    "## Figure 1: Tokenization using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c77fe69d-daff-44d9-8231-9b3b922adac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aliha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aliha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aliha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'nlp',\n",
       " 'exciting',\n",
       " 'field',\n",
       " 'combine',\n",
       " 'linguistics',\n",
       " 'computer',\n",
       " 'science']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "sample_text = \"Natural language processing (NLP) is an exciting field. It combines linguistics and computer science.\"\n",
    "nltk_tokens = word_tokenize(sample_text)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk_processed = [lemmatizer.lemmatize(token.lower()) for token in nltk_tokens if token.lower() not in stop_words and token.isalpha()]\n",
    "nltk_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e45352a-3a40-4e1c-8409-71d817eadc3f",
   "metadata": {},
   "source": [
    "## Figure 2: Tokenization using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3804f8a-33b5-4388-8f82-7cec4179c6c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'nlp',\n",
       " 'exciting',\n",
       " 'field',\n",
       " 'combine',\n",
       " 'linguistic',\n",
       " 'computer',\n",
       " 'science']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(sample_text)\n",
    "spacy_processed = [token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
    "spacy_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2556bf-d673-41d2-8ef9-151c91317568",
   "metadata": {},
   "source": [
    "## Figure 3: Named Entity Recognition output using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a236bc62-5b32-4b33-a12e-411771c93c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Apple (ORG)\n",
      " - U.K. (GPE)\n",
      " - $1 billion (MONEY)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking at buying \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.K.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " startup for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $1 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "ner_text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
    "doc = nlp(ner_text)\n",
    "\n",
    "# Print Named Entities\n",
    "for ent in doc.ents:\n",
    "    print(f\" - {ent.text} ({ent.label_})\")\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd5ec4d-4779-448c-a9f0-2ca7357ac59d",
   "metadata": {},
   "source": [
    "## Figure 4: Word Embeddings using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d49b00c-f28c-4665-b902-8adbe6341b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3291, -0.5040, -0.2125,  ..., -0.6529,  0.0756,  0.8283],\n",
       "         [-0.6841, -0.8352, -0.4526,  ..., -0.3956,  0.7534,  0.2176],\n",
       "         [-0.4530, -0.6353,  0.1921,  ..., -0.1519, -0.0703,  0.7846],\n",
       "         ...,\n",
       "         [-0.4442, -0.0778, -0.1558,  ..., -0.5368, -0.3660,  0.4046],\n",
       "         [ 0.5656, -0.0401, -0.7370,  ...,  0.2329, -0.5112, -0.3431],\n",
       "         [ 0.1879, -0.1038, -0.3582,  ...,  0.6637, -1.0281, -0.1297]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "sample_sentence = \"This is a sample sentence for embedding extraction.\"\n",
    "inputs = tokenizer(sample_sentence, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "embeddings = outputs.last_hidden_state\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790f8bf5-7bb6-495a-a651-a43ffb1a70f0",
   "metadata": {},
   "source": [
    "## Figure 5: Sentiment Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "413611f9-fa29-4c65-bd26-74a90be6797c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\aliha\\AppData\\Roaming\\nltk_data...\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer-based Sentiment Analysis:\n",
      "Sentence: I absolutely love this product! It works perfectly.\n",
      "Result: [{'label': 'POSITIVE', 'score': 0.9998786449432373}]\n",
      "\n",
      "Sentence: The service was terrible. I'm never coming back!\n",
      "Result: [{'label': 'NEGATIVE', 'score': 0.9989019632339478}]\n",
      "\n",
      "Sentence: The experience was okay, not the best, but not the worst either.\n",
      "Result: [{'label': 'NEGATIVE', 'score': 0.9642871618270874}]\n",
      "\n",
      "\n",
      "Traditional (Vader) Sentiment Analysis:\n",
      "Sentence: I absolutely love this product! It works perfectly.\n",
      "Scores: {'neg': 0.0, 'neu': 0.358, 'pos': 0.642, 'compound': 0.8746}\n",
      "Vader Sentiment: Positive\n",
      "\n",
      "Sentence: The service was terrible. I'm never coming back!\n",
      "Scores: {'neg': 0.326, 'neu': 0.674, 'pos': 0.0, 'compound': -0.5255}\n",
      "Vader Sentiment: Negative\n",
      "\n",
      "Sentence: The experience was okay, not the best, but not the worst either.\n",
      "Scores: {'neg': 0.128, 'neu': 0.527, 'pos': 0.345, 'compound': 0.5729}\n",
      "Vader Sentiment: Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "nltk.download(\"vader_lexicon\")\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "sentences = [\n",
    "    \"I absolutely love this product! It works perfectly.\",\n",
    "    \"The service was terrible. I'm never coming back!\",\n",
    "    \"The experience was okay, not the best, but not the worst either.\"\n",
    "]\n",
    "print(\"Transformer-based Sentiment Analysis:\")\n",
    "for sentence in sentences:\n",
    "    result = sentiment_pipeline(sentence)\n",
    "    print(f\"Sentence: {sentence}\\nResult: {result}\\n\")\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "print(\"\\nTraditional (Vader) Sentiment Analysis:\")\n",
    "for sentence in sentences:\n",
    "    scores = sia.polarity_scores(sentence)\n",
    "    sentiment = \"Positive\" if scores[\"compound\"] > 0 else \"Negative\" if scores[\"compound\"] < 0 else \"Neutral\"\n",
    "    print(f\"Sentence: {sentence}\\nScores: {scores}\\nVader Sentiment: {sentiment}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e067026-dc28-48ca-b6b3-a906bbdd355f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8fcae16-acb6-4f95-a9c6-c4df3b4ebb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aliha\\anaconda3\\envs\\pythonn\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I absolutely love this product! It exceeded my expectations.\n",
      "Result: [{'label': 'POSITIVE', 'score': 0.9998810291290283}]\n",
      "\n",
      "Sentence: This is the worst experience I have ever had with a service.\n",
      "Result: [{'label': 'NEGATIVE', 'score': 0.9997770190238953}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "sentences = [\n",
    "    \"I absolutely love this product! It exceeded my expectations.\",\n",
    "    \"This is the worst experience I have ever had with a service.\"\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    result = sentiment_pipeline(sentence)\n",
    "    print(f\"Sentence: {sentence}\\nResult: {result}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acab841-a295-4678-a2ba-ce84faae1a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonn",
   "language": "python",
   "name": "pythonn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
